<!DOCTYPE html>
<html>
    <head>
        <!-- meta -->
        <meta charset="UTF-8">
        
        <!-- title -->
        <title> My Research </title>
        <style>
            img {
                margin: auto;
                display: block;
            }
         </style>
    </head>

    <body> 
        <h1>
            From NLP to Transformers and Vice Versa
        </h1>

        <h2>
            Outline
        </h2>
        <p>
            <ol>
                <li> Terminology I've Come Across</li>
                <li> My Big Picture Understanding of the Field </li>
                <li> Things I do not Understand</li>
                <li> Resources </li>
            </ol>
        </p>

        <h2> Terminology </h2>
        <ol>
            <li> Natural Language Processing(NLP): Having machines take written/spoken language as input, understand it, process it, and then transforming it to an output space. Examples include translation from written English to verbal French, determining the best response, autocorrect, smart assistants, etc.) </li>
            <li> Transduction:Transformation of input sequences to output sequences <sup> 1</sup> </li> 
            <li> Self-attention: defined in the "Attention is All You Need" publication as "attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence"</li>
            <li> Attention function: f(query, Set of key:value pairs) = Output </li>
            <li> Sequence-Aligned Recurrence: </li>
            <li> Recurrent Attention Mechanism</li>
            <li> Learned vector representation </li>
            <li> Residual Connection: Connection that factors in Prior Input into Existing Calculations. Due to the inherent nature of gradient descent, having a residual connection allows the gradients to course through a particular input more directly without a bunch of coefficients in the way.  </li>
            <li> Normalization: Standardizes </li>
            <li> Autoregressive: Predicts future values by relying on past values </li>      
            <li> Query, Key, values
                <p> There are some excellent explanations for these terms including <a href = "https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms"> this </a>  and <a href = "https://www.youtube.com/watch?v=OyFJWRnt_AY""> this</a>. Essentially, consider the example of information retrieval systems where you have a query which tries to map to the best results. In order to do so, there are certain descriptors or keys which you test against the query, the best of which will have its value selected and shown at the top of the search results. In the context of transformers, </p>
            </li>
        </ol>
       
        <h2> What I Understand Regarding Transformers: The Big Picture View</h2>
            <h3> The Architecture Details of the Original Paper "Attention is All You Need" </h3>
                <figure> 
                    <img src = "../images/transformer_model_architecture.PNG" alt = "Transformer Model Architecture(taken from 'Attention is All You Need'"> 
                    <br>
                    <figcaption>
                        <center>
                            <b> Figure 1: </b>Transformer Model Architecture(taken from "Attention is All You Need")
                        </center> </figcaption> 
                </figure>
                <p> Like RNNs, the transformer model is divided into an encoder and decoder section. The encoder has one set of inputs and the decoder has two sets of inputs(these will be described soon). The set of inputs that the encoder receives is simply the set of inputs that goes into the model(i.e. if you're trying to translate English to French, the inputs into the encoder will be whatever English sentence is fed into the model). Prior to being fed into the encoder, each word in the sentence will be mapped to a vector which represents their relative locations in a high-dimensional space(i.e. the vectors for dog and horse might be relatively similar to one another b/c they are animals). This embedding space/mapping can either be trained or taken from elsewhere. After each word in the sentence is mapped to a vector, they are then added to a positional vector which encapsulates information about the relative location of each word using sine and cosine functions. Then, the encoder takes the resultant vector which represents each word and attempts to spit out new vectors which represent each word's semantic relation(within the sentence as opposed to general meaning like in the embedding stage) to one another. It does this using a combination of multi-headed attention, residual connections, and a feedforward network. </p>
                <p> The decoder model is a bit more complex. Let's asume that we are doing English to French translation and we have predicted the first French word. We now want to predict the next word. We will again apply and add embedding(except for French) and positional encoding vectors with regards to the French word. We will then put this vector through 2 attention blocks. The first attention block will use a technique called masking(which I do not completely understand yet). The second attention block will take the results of the first attention block along with the results from the encoder and form assosciation vectors between the French and English words. These resultant association vectors will then go through the rest of the model and output a set of probabilities for the possible selections for the next potential word</p>
                <ol>
                    <li> Why the feedforward network? </li>
                    <li> Where do the QKVs come from? </li>
                    <li> Why is it called multi-headed attention? </li>
                    The query key and value vectors are split(so for example a 6-dimensional query vector can be split into 2 3-dimensional query vectors-i.e. the top half and bottom half). The self-attention blocks(i.e. Softmax(QK^T/sqrt(d_k))*V)
                    <li> How do we reference words that are in the French dictionary that are possible options for the next word? How do the output probabilities look like and how do they refer to a word? -> Can only be answered in practice. </li>
                    <li> I understand that the FFNs are used to make the attention vectors into a more digestible form. Is there any other way of interpreting this function that is less vague?</li>
                    <li> How does parallelization manifest in the FFNs?</li>
                    <li> Do the FFNs backprop? </li>
                    <li>How do the weight matrices for QKV and from attention network to FFNs get generated/trained?</li> 
                </ol>

                <h2> Resources </h2>
            <ul>
                <li> Rise of Transformers & Details of Transformers Architecture
                    <ul> 
                        <li> <a href = "https://www.youtube.com/watch?v=OyFJWRnt_AY"> U of Waterloo Lecture: Attention and Transformer Networks </a> </li>
                        <li> <a href = "https://www.youtube.com/watch?v=4Bdc55j80l8" >The AI Hacker - Illustrated Guide to Transformers Neural Network: A step by step explanation </a></li>
                        <li> <a href = "https://www.youtube.com/watch?v=TQQlZhbC5ps"> Excellent High-Level Overview of Transformers from TheCodeEmporium </li>
                        <li> https://github.com/anebz/papers </li>
                    </ul>
                </li>



                
            </ul>
        
        




        <h2> Source Material/Useful Links </h2>
        <ol>
            <li> https://aicorespot.io/an-intro-to-transduction-within-machine-learning/</li>

        </ol>
    </body>
</html>

<!-- <h2> Transformers for NLP: Attention is All You Need </h2>
        <p> The authors of this paper include the following: 
            <ol>
                <li> Ashish Vaswani </li> 
                <li> Noam Shazeer </li>
                <li> Niki Parmar </li>
                <li> Jakob Uszkoreit </li>
                <li> Llion Jones </li>
                <li> Aidan N. Gomez </li>
                <li> ≈Åukasz Kaiser </li>
                <li> Illia Polosukhin </li>
            </ol>
            <br>
            <br>
        </p> -->


        <!-- <h3> Why do Transformers Matter? </h3>
        In a nutshell, here are the advantages that transformers provide:
        <ol>
            <li> Does not use convolutions or recurrence - Saves on computational resources </li>
            <li> Self-Attention Mechanism: Comparisons of outputs/inputs that are found at large distances from one another in the sequence has a constant time cost as opposed to a logarithmic or linear cost. As Michael Phi(the AI Hacker) puts it, transformers have a practically "infinite reference window".  This is crucial because previous models of language learning had short-term memory deficits due to a lack of a global attention mechanism. In other words, as an input sequence got longer, the attention given to words in the beginning became so small that the information from those words could no longer be used. There were alterations made(hence LSTMs and GRUs) but those too have their limitations. </li>
            <li> Training: Backpropagation occurs for each word that is produced by the transformer. This is unlike an RNN where an entire sentence would have to be produced before backprop occurs. This is </li>
        </ol>

        Here are the disadvantages that transformers have and whether they are mitigated by another aspect of the transformer architecture.

        <ol>

        </ol>
        <h3> How do Transformers Work? </h3>
        <figure> 
            <img src = "../images/transformer_model_architecture.PNG" alt = "Transformer Model Architecture(taken from 'Attention is All You Need'"> 
            <br>
            <figcaption>
                <center>
                    <b> Figure 1: </b>Transformer Model Architecture(taken from "Attention is All You Need")
                </center> </figcaption> 
        </figure>

        Parts of a Transformer:
        <ol>
            <li> Input embedding: Takes Word as Input and uses a lookup table to generate a vector(learned-vector representation) </li>The
            <li> Positional Encodings: Used to encode information about the position of the words. Uses cosine and sine for odd and even time steps, respectively. The peaks and troughs of waves with different frequencies appear to correlate to different words and their relative positions but I still don't fully understand this. </li>
            <li> Encoder: f(Input sequence) -> Abstract continuous representation
                <ol>
                    <li>
                        Multi-headed attention: Applies self-attention. Queries and Keys are multiplied together to create a score which determines which words are important. Don't forget that the query and key multiplication is essentially vectors(in the form of matrices) dot multiplying together. Dot multiplying remember looks at the angle between two vectors. These scores are scaled down by dividing by the square root of the number of queries and keys(d<sub>k</sub>) and then subsequently further elongated(as in the distance between scores increases) thanks to the softmax function. Afterwards, the value vector is multiplied to the intermediate result. The purpose is essentially like creating a hidden state vector.
                        <figure> 
                            <img src = "../images/multi_headed_attention.PNG" alt = "Multi-headed Attention(taken from 'Attention is All You Need'"> 
                            <figcaption><b> Figure 2: </b>Multi-headed Attention(taken from "Attention is All You Need") </figcaption> 
                        </figure>
                    </li>
                    <li> Residual connection + Normalization: Original input & output of multi-headed attention are added together and normalized. </li>
                    <li> Pointwise Feed Forward Neural Network - Multiple layers with a ReLU activation function </li>
                </ol>
            </li>
            <li> Decoder: Autoregressive 
                <ol>
                    <li>
                        Multi-headed attention: Refer to description from Encoders. However, note that unlike in encoders, the first multi-headed attention mechanism in decoders use a principle known as masking. Like the word suggests, masking involves shielding some known elements while preserving others. When tabulating the scores for the words, it is not possible for a word at position x to know the word at position x + 1. When making predictions when the last known word is at position x, you only have access to words from positions 1 to x. Here is an example of what masking means in this context: 
                    
                    </li>
                    <li> Linear Classifier: Number of classes depends on the number of words in the transformer's "dictionary" of potential words</li>
                </ol>
            </li>
        </ol>
        
        <br>
        <br> -->